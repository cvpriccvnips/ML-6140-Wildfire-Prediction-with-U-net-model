{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a310e26e-315a-4626-b76e-0c4e53259e88",
   "metadata": {},
   "source": [
    "## This notebook contains prediction part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bb881-feb6-4983-b685-0f409e38de31",
   "metadata": {},
   "source": [
    "#### run code in common notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0f3524-d03e-4f39-a304-5f8a6510ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 02:06:19.305023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "<class 'list'>\n",
      "['NDVI', 'Lai', 'SoilMoi00_10cm_tavg', 'LST_Day_1km', 'total_precipitation', 'u_component_of_wind_10m', 'v_component_of_wind_10m', 'T21']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "%run 0_common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17046027-1887-4355-8bf1-1dcb654bef28",
   "metadata": {},
   "source": [
    "### Predict on evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9adb23-cb10-47ca-957a-ec7e58f88c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 5\u001b[0m ev \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Predict on the evaluation dataset with the specified number of steps\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mpredict(x\u001b[38;5;241m=\u001b[39mev)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "ev = evaluation.take(100)\n",
    "# Predict on the evaluation dataset with the specified number of steps\n",
    "predictions = m.predict(x=ev)\n",
    "\n",
    "print(len(predictions[0]))\n",
    "print(len(predictions[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8630b23-28e8-4327-b30c-7b72f7764311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ee.image.Image object at 0x7fc7845536d0>, <ee.image.Image object at 0x7fc784553040>, <ee.image.Image object at 0x7fc7846e7580>, <ee.image.Image object at 0x7fc7846f8430>, <ee.image.Image object at 0x7fc78459c790>, <ee.image.Image object at 0x7fc78459cf70>, <ee.image.Image object at 0x7fc78459d5d0>]\n"
     ]
    }
   ],
   "source": [
    "feature_img_lists = []\n",
    "date_start_str = '2019-07-01'\n",
    "pred_start = ee.Date(date_start_str)\n",
    "pred_end = pred_start.advance(4, 'week')\n",
    "\n",
    "for feature in feature_list:\n",
    "  img_col = getImageCollection(feature['name'],pred_region_geometry, feature['band'])\n",
    "  pred_img = img_col.filterDate(pred_start, pred_end).median().unitScale(feature['min'], feature['max'])\n",
    "  feature_img_lists += [pred_img]\n",
    "print(feature_img_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec8b6d-783e-43e2-afe5-09a863f72bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image = feature_img_lists[0]\n",
    "for idx in range(1, len(feature_img_lists)):\n",
    "  # Combine all bands sampled from the same time to featureStack\n",
    "  predict_image = ee.Image.cat([predict_image, feature_img_lists[idx]]).float()\n",
    "# predict_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6101614-2fdb-4244-bef9-acbb54c36ce3",
   "metadata": {
    "id": "e6TV1dBv2qZi"
   },
   "outputs": [],
   "source": [
    "def doExport(predict_image, out_file_name, kernel_buffer, region):\n",
    "    \"\"\"Run the image export task.  Block until complete.\n",
    "    \"\"\"\n",
    "    # Define the export task using the Earth Engine Python API.\n",
    "    task = ee.batch.Export.image.toCloudStorage(\n",
    "        image=predict_image,  # Select specific bands from the 'featureStack' Image.\n",
    "        description=out_file_name,  # Description for the export task.\n",
    "        bucket=OUTPUT_BUCKET,  # The destination Cloud Storage bucket.\n",
    "        fileNamePrefix=FOLDER + '/' + out_file_name,  # File name prefix for the exported images.\n",
    "        region=region.getInfo()['coordinates'],  # The bounding region for the export task.\n",
    "        scale=SAMPLE_RESOLUTION,  # The spatial resolution in meters for the exported images.\n",
    "        fileFormat='TFRecord',  # File format for the exported images (TensorFlow Record).\n",
    "        maxPixels=1e10,  # Maximum number of pixels allowed for the export task.\n",
    "        formatOptions={\n",
    "            'patchDimensions': KERNEL_SHAPE,  # Dimensions of the image patches for segmentation.\n",
    "            'kernelSize': kernel_buffer,  # Size of the kernel buffer for segmentation.\n",
    "            'compressed': True,  # Enable compression for the exported images.\n",
    "            'maxFileSize': 104857600  # Maximum file size for each exported image in bytes.\n",
    "        }\n",
    "    )\n",
    "    print(FOLDER + '/' + out_file_name)\n",
    "    # Start the export task.\n",
    "    task.start()\n",
    "\n",
    "    # Block until the task completes.\n",
    "    print('Running image export to Cloud Storage...')\n",
    "    import time\n",
    "    while task.active():\n",
    "        print('Polling for task (id: {}).'.format(task.id))\n",
    "        time.sleep(30)\n",
    "\n",
    "    # Check if the export task encountered an error.\n",
    "    if task.status()['state'] != 'COMPLETED':\n",
    "        print(f\"Error with image export {task.status()['state']}\")\n",
    "    else:\n",
    "        print('Image export completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f279d-8ee1-4ba5-93f8-2e9ab8d83cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doExport(predict_image, out_file_name, KERNEL_SIZE, pred_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77688b65-4c1e-4963-9b83-d182347d08c1",
   "metadata": {
    "id": "drMLDp2LKt1N"
   },
   "source": [
    "**!remember to change user_folder to your own GEE username**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776028b-b182-48dc-8bea-09ec589db82f",
   "metadata": {
    "id": "YijoX3Rb5uaK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def doPrediction(out_image_base, in_image_bucket, out_image_bucket, user_folder, kernel_buffer):\n",
    "#   \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
    "#   \"\"\"\n",
    "\n",
    "#   print('Looking for TFRecord files...')\n",
    "\n",
    "#   # Get a list of all the files in the output bucket.\n",
    "#   filesList = !gsutil ls 'gs://'{in_image_bucket}'/'{FOLDER}\n",
    "\n",
    "#   # Get only the files generated by the image export.\n",
    "#   exportFilesList = [s for s in filesList if out_image_base in s]\n",
    "\n",
    "#   # Get the list of image files and the JSON mixer file.\n",
    "#   imageFilesList = []\n",
    "#   jsonFile = None\n",
    "#   for f in exportFilesList:\n",
    "#     if f.endswith('.tfrecord.gz'):\n",
    "#       imageFilesList.append(f)\n",
    "#     elif f.endswith('.json'):\n",
    "#       jsonFile = f\n",
    "\n",
    "#   # Make sure the files are in the right order.\n",
    "#   imageFilesList.sort()\n",
    "\n",
    "#   from pprint import pprint\n",
    "#   pprint(imageFilesList)\n",
    "#   print(jsonFile)\n",
    "\n",
    "#   import json\n",
    "#   # Load the contents of the mixer file to a JSON object.\n",
    "#   jsonText = !gsutil cat {jsonFile}\n",
    "#   # Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "#   mixer = json.loads(jsonText.nlstr)\n",
    "#   pprint(mixer)\n",
    "#   patches = mixer['totalPatches']\n",
    "\n",
    "#   # Get set up for prediction.\n",
    "#   x_buffer = int(kernel_buffer[0] / 2)\n",
    "#   y_buffer = int(kernel_buffer[1] / 2)\n",
    "\n",
    "#   buffered_shape = [\n",
    "#       KERNEL_SHAPE[0] + kernel_buffer[0],\n",
    "#       KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
    "\n",
    "#   imageColumns = [\n",
    "#     tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
    "#       for k in BANDS\n",
    "#   ]\n",
    "\n",
    "#   imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
    "\n",
    "#   def parse_image(example_proto):\n",
    "#     return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
    "\n",
    "#   def toTupleImage(inputs):\n",
    "#     inputsList = [inputs.get(key) for key in BANDS]\n",
    "#     stacked = tf.stack(inputsList, axis=0)\n",
    "#     stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "#     return stacked\n",
    "\n",
    "#    # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
    "#   imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "#   imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
    "#   imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
    "\n",
    "#   # Perform inference.\n",
    "#   print('Running predictions...')\n",
    "#   predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
    "#   # print(predictions[0])\n",
    "\n",
    "#   print('Writing predictions...')\n",
    "#   out_image_file = 'gs://' + out_image_bucket + '/' + FOLDER + '/' + out_image_base  + '.TFRecord'\n",
    "#   writer = tf.io.TFRecordWriter(out_image_file)\n",
    "#   patches = 0\n",
    "#   for predictionPatch in predictions:\n",
    "#     print('Writing patch ' + str(patches) + '...')\n",
    "#     predictionPatch = predictionPatch[\n",
    "#         x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
    "\n",
    "#     # Create an example.\n",
    "#     example = tf.train.Example(\n",
    "#       features=tf.train.Features(\n",
    "#         feature={\n",
    "#           'impervious': tf.train.Feature(\n",
    "#               float_list=tf.train.FloatList(\n",
    "#                   value=predictionPatch.flatten()))\n",
    "#         }\n",
    "#       )\n",
    "#     )\n",
    "#     # Write the example.\n",
    "#     writer.write(example.SerializeToString())\n",
    "#     patches += 1\n",
    "\n",
    "#   writer.close()\n",
    "\n",
    "#   # Start the upload.\n",
    "#   out_image_asset = user_folder + '/' + out_image_base \n",
    "#   !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee64a3a-0611-4d43-8ba5-21702b289bcc",
   "metadata": {},
   "source": [
    "for debug(change to your own username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f2925-0ab3-49da-a313-a462d965963e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for TFRecord files...\n",
      "('imageFilesList '\n",
      " \"['gs://6140-output-bucket/fcnn-demo/pred_Hanceville_2019-07-01.tfrecord.gz']\")\n",
      "{'patchDimensions': [256, 256],\n",
      " 'patchesPerRow': 2,\n",
      " 'projection': {'affine': {'doubleMatrix': [0.008983152841195215,\n",
      "                                            0.0,\n",
      "                                            -125.76413977673302,\n",
      "                                            0.0,\n",
      "                                            -0.008983152841195215,\n",
      "                                            56.728610192147784]},\n",
      "                'crs': 'EPSG:4326'},\n",
      " 'totalPatches': 2}\n",
      "Running predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 20:17:00.729104: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-08-12 20:17:00.953701: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: v_component_of_wind_10m.  Can't parse serialized Example.\n",
      "2023-08-12 20:17:00.953770: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Key: v_component_of_wind_10m.  Can't parse serialized Example.\n",
      "\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n",
      "2023-08-12 20:17:00.972742: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at example_parsing_ops.cc:98 : INVALID_ARGUMENT: Key: v_component_of_wind_10m.  Can't parse serialized Example.\n",
      "2023-08-12 20:17:00.972784: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Key: v_component_of_wind_10m.  Can't parse serialized Example.\n",
      "\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: v_component_of_wind_10m.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m   get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearthengine upload image --asset_id=\u001b[39m\u001b[38;5;132;01m{out_image_asset}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{out_image_file}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{jsonFile}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Run the prediction.\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43mdoPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_kernel_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_region\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 80\u001b[0m, in \u001b[0;36mdoPrediction\u001b[0;34m(out_image_base, user_folder, kernel_buffer, region)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Perform inference.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning predictions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print(predictions[0])\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWriting predictions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Key: v_component_of_wind_10m.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "user_folder = 'users/jindacz' # INSERT YOUR FOLDER HERE.\n",
    "pred_kernel_buffer = [128, 128]\n",
    "# date_start_str = '2019-07-01'\n",
    "out_file_name = f'pred_Hanceville_{date_start_str}'\n",
    "pred_region = ee.Geometry.Polygon(\n",
    "    [[[-120.09501953125, 53.81667309827828], \n",
    "    [-120.09501953125, 56.69527587549462], \n",
    "    [-125.76396484375, 56.69527587549462],     \n",
    "    [-125.76396484375, 53.81667309827828]]],  None, False)\n",
    "FOLDER = 'fcnn-demo'\n",
    "\n",
    "def doPrediction(out_image_base, user_folder, kernel_buffer, region):\n",
    "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
    "  \"\"\"\n",
    "\n",
    "  print('Looking for TFRecord files...')\n",
    "\n",
    "  # Get a list of all the files in the output bucket.\n",
    "  filesList = !gsutil ls 'gs://'{OUTPUT_BUCKET}'/'{FOLDER}\n",
    "\n",
    "  # Get only the files generated by the image export.\n",
    "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
    "\n",
    "  # Get the list of image files and the JSON mixer file.\n",
    "  imageFilesList = []\n",
    "  jsonFile = None\n",
    "  for f in exportFilesList:\n",
    "    if f.endswith('.tfrecord.gz'):\n",
    "      imageFilesList.append(f)\n",
    "    elif f.endswith('.json'):\n",
    "      jsonFile = f\n",
    "\n",
    "  # Make sure the files are in the right order.\n",
    "  imageFilesList.sort()\n",
    "\n",
    "  from pprint import pprint\n",
    "  pprint(f'imageFilesList {imageFilesList}')\n",
    "  # print(f'jsonFile {jsonFile}')\n",
    "\n",
    "  import json\n",
    "  # Load the contents of the mixer file to a JSON object.\n",
    "  jsonText = !gsutil cat {jsonFile}\n",
    "  # print(f'jsonText{jsonText}')\n",
    "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "  mixer = json.loads(jsonText.nlstr)\n",
    "  pprint(mixer)\n",
    "  patches = mixer['totalPatches']\n",
    "\n",
    "  # Get set up for prediction.\n",
    "  x_buffer = int(kernel_buffer[0] / 2)\n",
    "  y_buffer = int(kernel_buffer[1] / 2)\n",
    "\n",
    "  buffered_shape = [\n",
    "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
    "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
    "\n",
    "  imageColumns = [\n",
    "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
    "      for k in BANDS\n",
    "  ]\n",
    "\n",
    "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
    "\n",
    "  def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
    "\n",
    "  def toTupleImage(inputs):\n",
    "    inputsList = [inputs.get(key) for key in BANDS]\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    return stacked\n",
    "\n",
    "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
    "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
    "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
    "\n",
    "  # Perform inference.\n",
    "  print('Running predictions...')\n",
    "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
    "  # print(predictions[0])\n",
    "\n",
    "  print('Writing predictions...')\n",
    "  # out_image_base = 'shufeitest123'\n",
    "  out_image_file = 'gs://' + OUTPUT_BUCKET + '/' + FOLDER + '/' + out_image_base + '.TFRecord'\n",
    "  \n",
    "  print(out_image_file)\n",
    "  writer = tf.io.TFRecordWriter(out_image_file)\n",
    "  patches = 0\n",
    "    \n",
    "  for predictionPatch in predictions:\n",
    "    \n",
    "    print('Writing patch ' + str(patches) + '...')\n",
    "    predictionPatch = predictionPatch[\n",
    "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
    "    \n",
    "    # print(f'prediction patch {predictionPatch}')\n",
    "    # Create an example.\n",
    "    example = tf.train.Example(\n",
    "      features=tf.train.Features(\n",
    "        feature={\n",
    "          'impervious': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=predictionPatch.flatten()))\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    # Write the example.\n",
    "    writer.write(example.SerializeToString())\n",
    "    print('finish writing patch ' + str(patches) + '...')\n",
    "    patches += 1\n",
    "\n",
    "\n",
    "  #writer.close()\n",
    "  print(f'fin writing')\n",
    "\n",
    "  # Start the upload.\n",
    "  out_image_asset = user_folder + '/' + out_image_base\n",
    "  !earthengine upload image --asset_id={out_image_asset} {out_image_file} {jsonFile}\n",
    "\n",
    "# Run the prediction.\n",
    "doPrediction(out_file_name, user_folder, pred_kernel_buffer, pred_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51c064-09d9-4d7f-9fb3-e637bff10492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install tensorflow-io \n",
    "# !python -m pip uninstall -y tensorflow-io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c839f4-144e-475e-8d71-72b8cdf94993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doPrediction(out_file_name, OUTPUT_BUCKET, DATA_BUCKET, FOLDER, kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877cc4d-b664-4a22-a47d-47c33a19f6d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTC7eZoc201K",
    "outputId": "92790209-2c43-4547-da18-d776f91ac5fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Run the prediction.\n",
    "print(user_folder)\n",
    "print(out_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b07e9-85ca-4dbe-b884-5c7e7c5e4d1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "collapsed": true,
    "id": "cehGLdSJ249z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bcc4ce4d-3502-43c1-a940-df313e35e9fb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_17a0cd048618ddade563053436fc3830 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_17a0cd048618ddade563053436fc3830&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_17a0cd048618ddade563053436fc3830 = L.map(\n",
       "                &quot;map_17a0cd048618ddade563053436fc3830&quot;,\n",
       "                {\n",
       "                    center: [53.81667309827828, -120.09501953125],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_9ff86c02f48c4d1b71f5e0b57c442666 = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_17a0cd048618ddade563053436fc3830);\n",
       "        \n",
       "    \n",
       "            var tile_layer_08d6c5003af9f494243c3f044ed7417b = L.tileLayer(\n",
       "                &quot;https://earthengine.googleapis.com/v1/projects/earthengine-legacy/maps/c0463c22a22823829a29bba74ad0d755-0daeacc5299bc563e13d633bf6be8e15/tiles/{z}/{x}/{y}&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Map Data \\u0026copy; \\u003ca href=\\&quot;https://earthengine.google.com/\\&quot;\\u003eGoogle Earth Engine\\u003c/a\\u003e&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_17a0cd048618ddade563053436fc3830);\n",
       "        \n",
       "    \n",
       "            var layer_control_684d3b33307f0bb687179dbdf1ccb115 = {\n",
       "                base_layers : {\n",
       "                    &quot;openstreetmap&quot; : tile_layer_9ff86c02f48c4d1b71f5e0b57c442666,\n",
       "                },\n",
       "                overlays :  {\n",
       "                    &quot;predicted impervious&quot; : tile_layer_08d6c5003af9f494243c3f044ed7417b,\n",
       "                },\n",
       "            };\n",
       "            L.control.layers(\n",
       "                layer_control_684d3b33307f0bb687179dbdf1ccb115.base_layers,\n",
       "                layer_control_684d3b33307f0bb687179dbdf1ccb115.overlays,\n",
       "                {&quot;autoZIndex&quot;: true, &quot;collapsed&quot;: true, &quot;position&quot;: &quot;topright&quot;}\n",
       "            ).addTo(map_17a0cd048618ddade563053436fc3830);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7fc78459ea10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out_image = ee.Image(user_folder + '/' + bj_image_base)\n",
    "out_image = ee.Image(user_folder + '/' + out_file_name)\n",
    "mapid = out_image.getMapId({'min': 0, 'max': 1})\n",
    "map = folium.Map(location=[53.81667309827828, -120.09501953125])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "    overlay=True,\n",
    "    name='predicted impervious',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59476a9e-f3e5-4880-a913-a20adf10b2aa",
   "metadata": {},
   "source": [
    "export prediction to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f663c4-5a05-47a7-b13d-0d0d353d8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_values = out_image.reduceRegion(\n",
    "    reducer=ee.Reducer.toList(),\n",
    "    geometry=pred_region,\n",
    "    scale=926.625433055833,  # The scale from the image's 'crs_transform'\n",
    ")\n",
    "\n",
    "impervious_list = pred_values.get('impervious').getInfo()\n",
    "\n",
    "# Convert the list of pixel values to a pandas DataFrame\n",
    "df = pd.DataFrame(impervious_list, columns=['prediction'])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('prediction_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a048d-0e2e-4a9f-8e8d-3f399a600dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pred_values.get('impervious'))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
